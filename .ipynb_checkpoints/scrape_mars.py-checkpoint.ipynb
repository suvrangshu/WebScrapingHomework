{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-1-e1f7346d7bc2>, line 110)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e1f7346d7bc2>\"\u001b[1;36m, line \u001b[1;32m110\u001b[0m\n\u001b[1;33m    return final_output\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "#mission_to_mars.ipynb\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "#executable_path = {'executable_path': '/Users/sghosh/Documents/Suv/Personal/Berkeley/chromedriver.exe'}\n",
    "executable_path = {\"executable_path\": \"drivers/chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "\n",
    "#url = 'https://mars.nasa.gov/news/'\n",
    "#pic_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "#t_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "#fact_url = 'http://space-facts.com/mars/'\n",
    "#hem_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    \n",
    "    # url = https://mars.nasa.gov/news/' \n",
    "    url = 'https://mars.nasa.gov/news/'\n",
    "    browser.visit(url)\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    title = soup.find('div', class_='content_title') # gets 1st title\n",
    "    news_title = title.text\n",
    "    title_p = soup.find('div', class_='article_teaser_body') # gets paragraph text\n",
    "    #get news details\n",
    "    news_p = title_p.text\n",
    "    \n",
    "    ## Section to get image\n",
    "    pic_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "    browser.visit(pic_url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(browser.html, \"html.parser\")\n",
    "    \n",
    "    #get image\n",
    "    image = soup.find('img', class_='main_image') # gets 1st title\n",
    "    #get the image path\n",
    "    featured_image = image[\"src\"]\n",
    "    #creating full path for the image url:\n",
    "    featured_image_url = 'https://www.jpl.nasa.gov' + featured_image\n",
    "    \n",
    "    #Twitter\n",
    "    twit_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "    browser.visit(twit_url)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(browser.html, \"html.parser\")\n",
    "    twit = soup.find('p', class_='tweet-text') # gets 1st title\n",
    "    mars_weather= twit.text\n",
    "    #spliting to get rid of the - pic.twitter.com/WlR4gr8gpC\n",
    "    x = mars_weather.split(\"\\n\")\n",
    "    #picking only the part required\n",
    "    mars_weather = x[0]\n",
    "    \n",
    "    #Fact Url :\n",
    "    fact_url = 'http://space-facts.com/mars/'\n",
    "    time.sleep(1)\n",
    "    tables = pd.read_html(fact_url)\n",
    "    \n",
    "    #put column head\n",
    "    df = tables[0]\n",
    "    df.columns = ['Mars_detail', 'Values']\n",
    "    \n",
    "    #pandas to HTML\n",
    "    fact_table = df.to_html()\n",
    "\n",
    "    #clean up new lines\n",
    "    fact_table = fact_table.replace('\\n', '')\n",
    "    \n",
    "    #Mars Hemispheres\n",
    "\n",
    "    hem_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    browser.visit(hem_url)\n",
    "    time.sleep(4) #giving time for loading page\n",
    "    hemisphere_image_urls = []\n",
    "    h_soup = BeautifulSoup(browser.html, 'lxml')\n",
    "    hemis_check = h_soup.find_all(class_='description')\n",
    "    \n",
    "    for results in hemis_check:\n",
    "        h1 = results.h3.text\n",
    "        #href_image = h1['href']\n",
    "        browser.click_link_by_partial_text(h1)\n",
    "        time.sleep(1)\n",
    "        hemis_temp_soup = BeautifulSoup(browser.html, 'lxml')\n",
    "        image_link = hemis_temp_soup.find('a',target=\"_blank\" )\n",
    "        href_image = image_link[\"href\"]\n",
    "        hemisphere_image_urls.append({'title':h1, 'img_url':href_image})\n",
    "        browser.visit(hem_url) # takes back to home\n",
    "        \n",
    "    #close browsers\n",
    "    browser.quit()\n",
    "    \n",
    "    \n",
    "    #Final Output dictionary\n",
    "    final_output = dict (\n",
    "        top_news_head = news_title,\n",
    "        news_detail = news_p,\n",
    "        featured_image = featured_image_url,\n",
    "        weather = mars_weather,\n",
    "        facts_table = fact_table,\n",
    "        hemisphere_images = hemisphere_image_urls\n",
    "    )\n",
    "    \n",
    "    return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
